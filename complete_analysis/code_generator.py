"""
Analysis Code Generation for E-commerce Data
Generates reproducible, production-ready analysis code
"""

import pandas as pd
import numpy as np
import os
import json
import warnings
import sys
from datetime import datetime

warnings.filterwarnings('ignore')

# Set UTF-8 encoding for Windows console
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

class AnalysisCodeGenerator:
    """Generate comprehensive analysis code"""

    def __init__(self, data_dir='data_storage/', output_dir='complete_analysis/generated_code/'):
        self.data_dir = data_dir
        self.output_dir = output_dir
        self.datasets = {}

        # Create output directory
        os.makedirs(output_dir, exist_ok=True)

    def generate_data_preprocessing_code(self):
        """Generate data preprocessing module"""
        print("\nüìù Generating data preprocessing code...")

        code = '''"""
Data Preprocessing Module for E-commerce Analysis
Handles data loading, cleaning, and preparation
"""

import pandas as pd
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class DataPreprocessor:
    """Data preprocessing and cleaning"""

    def __init__(self, data_dir='data_storage/'):
        self.data_dir = data_dir
        self.data = {}

    def load_all_data(self):
        """Load all datasets"""
        print("Loading datasets...")

        datasets = ['Orders.csv', 'Customers.csv', 'Order Items.csv',
                   'Order Payments.csv', 'Products.csv', 'Reviews.csv',
                   'Sellers.csv', 'Categories.csv']

        for dataset in datasets:
            try:
                df = pd.read_csv(f"{self.data_dir}/{dataset}")
                self.data[dataset.replace('.csv', '')] = df
                print(f"  ‚úì Loaded {dataset}: {df.shape[0]:,} rows")
            except Exception as e:
                print(f"  ‚úó Error loading {dataset}: {e}")

        return self.data

    def clean_orders(self):
        """Clean and preprocess orders data"""
        if 'Orders' not in self.data:
            return None

        df = self.data['Orders'].copy()

        # Convert date columns
        date_cols = ['order_purchase_timestamp', 'order_approved_at',
                    'order_delivered_carrier_date', 'order_delivered_customer_date',
                    'order_estimated_delivery_date']

        for col in date_cols:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')

        # Extract features
        df['order_year'] = df['order_purchase_timestamp'].dt.year
        df['order_month'] = df['order_purchase_timestamp'].dt.month
        df['order_day_of_week'] = df['order_purchase_timestamp'].dt.dayofweek
        df['order_hour'] = df['order_purchase_timestamp'].dt.hour

        # Calculate delivery time
        if 'order_delivered_customer_date' in df.columns:
            df['delivery_days'] = (
                df['order_delivered_customer_date'] - df['order_purchase_timestamp']
            ).dt.days

        # Handle missing values
        df['order_delivered_customer_date'].fillna(df['order_estimated_delivery_date'], inplace=True)

        self.data['Orders_clean'] = df
        return df

    def clean_customers(self):
        """Clean and preprocess customers data"""
        if 'Customers' not in self.data:
            return None

        df = self.data['Customers'].copy()

        # Encode categorical variables
        if 'customer_state' in df.columns:
            df['customer_state_code'] = df['customer_state'].astype('category').cat.codes

        self.data['Customers_clean'] = df
        return df

    def clean_order_items(self):
        """Clean and preprocess order items data"""
        if 'Order Items' not in self.data:
            return None

        df = self.data['Order Items'].copy()

        # Calculate total item value
        if 'price' in df.columns and 'freight_value' in df.columns:
            df['total_item_value'] = df['price'] + df['freight_value']

        self.data['Order_Items_clean'] = df
        return df

    def clean_payments(self):
        """Clean and preprocess payments data"""
        if 'Order Payments' not in self.data:
            return None

        df = self.data['Order Payments'].copy()

        # Flag installment payments
        if 'payment_installments' in df.columns:
            df['is_installment'] = df['payment_installments'] > 1

        self.data['Payments_clean'] = df
        return df

    def clean_reviews(self):
        """Clean and preprocess reviews data"""
        if 'Reviews' not in self.data:
            return None

        df = self.data['Reviews'].copy()

        # Convert date columns
        if 'review_creation_date' in df.columns:
            df['review_creation_date'] = pd.to_datetime(df['review_creation_date'], errors='coerce')

        if 'review_answer_timestamp' in df.columns:
            df['review_answer_timestamp'] = pd.to_datetime(df['review_answer_timestamp'], errors='coerce')

        # Create sentiment score (simple: 4-5 positive, 3 neutral, 1-2 negative)
        if 'review_score' in df.columns:
            df['sentiment'] = df['review_score'].apply(
                lambda x: 'positive' if x >= 4 else 'neutral' if x == 3 else 'negative'
            )

        # Comment length
        if 'review_comment_message' in df.columns:
            df['comment_length'] = df['review_comment_message'].str.len().fillna(0)

        self.data['Reviews_clean'] = df
        return df

    def merge_datasets(self):
        """Merge related datasets"""
        print("Merging datasets...")

        if 'Orders_clean' in self.data and 'Order_Items_clean' in self.data:
            # Merge orders with items
            orders_items = pd.merge(
                self.data['Orders_clean'],
                self.data['Order_Items_clean'],
                on='order_id',
                how='inner'
            )
            self.data['Orders_Items_merged'] = orders_items
            print(f"  ‚úì Orders + Items merged: {len(orders_items):,} rows")

        if 'Orders_clean' in self.data and 'Customers_clean' in self.data:
            # Merge orders with customers
            orders_customers = pd.merge(
                self.data['Orders_clean'],
                self.data['Customers_clean'],
                on='customer_id',
                how='left'
            )
            self.data['Orders_Customers_merged'] = orders_customers
            print(f"  ‚úì Orders + Customers merged: {len(orders_customers):,} rows")

        return self.data

    def get_clean_data(self):
        """Get all cleaned and merged data"""
        return self.data

    def export_cleaned_data(self, output_dir='complete_analysis/generated_code/'):
        """Export cleaned datasets"""
        import os
        os.makedirs(output_dir, exist_ok=True)

        for name, df in self.data.items():
            if 'clean' in name.lower() or 'merged' in name.lower():
                path = f"{output_dir}/{name}.csv"
                df.to_csv(path, index=False)
                print(f"  ‚úì Exported {name}")

# Usage example
if __name__ == "__main__":
    preprocessor = DataPreprocessor()
    preprocessor.load_all_data()
    preprocessor.clean_orders()
    preprocessor.clean_customers()
    preprocessor.clean_order_items()
    preprocessor.clean_payments()
    preprocessor.clean_reviews()
    preprocessor.merge_datasets()
    preprocessor.export_cleaned_data()
'''

        # Save code
        code_path = os.path.join(self.output_dir, 'data_preprocessing.py')
        with open(code_path, 'w', encoding='utf-8') as f:
            f.write(code)

        print(f"  ‚úì Saved to: {code_path}")
        return code_path

    def generate_quality_checks_code(self):
        """Generate data quality validation code"""
        print("\nüìù Generating quality checks code...")

        code = '''"""
Data Quality Validation Module
Performs comprehensive data quality checks and validation
"""

import pandas as pd
import numpy as np

class DataQualityValidator:
    """Validate data quality across all datasets"""

    def __init__(self):
        self.results = {}

    def check_completeness(self, df, dataset_name):
        """Check data completeness (missing values)"""
        total_cells = df.shape[0] * df.shape[1]
        missing_cells = df.isnull().sum().sum()
        completeness_score = (1 - missing_cells / total_cells) * 100

        return {
            'dataset': dataset_name,
            'completeness_score': completeness_score,
            'missing_pct': (missing_cells / total_cells) * 100,
            'columns_with_missing': df.isnull().sum().to_dict()
        }

    def check_accuracy(self, df, dataset_name):
        """Check data accuracy (outliers, data types)"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        outlier_info = {}

        for col in numeric_cols:
            data = df[col].dropna()
            if len(data) > 0:
                Q1 = data.quantile(0.25)
                Q3 = data.quantile(0.75)
                IQR = Q3 - Q1

                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR

                outliers = ((data < lower_bound) | (data > upper_bound)).sum()
                outlier_info[col] = int(outliers)

        return {
            'dataset': dataset_name,
            'columns_with_outliers': outlier_info
        }

    def check_consistency(self, df, dataset_name):
        """Check data consistency (duplicates, IDs)"""
        duplicate_rows = df.duplicated().sum()

        # Check ID columns
        id_cols = [col for col in df.columns if 'id' in col.lower()]
        id_issues = {}

        for col in id_cols:
            null_ids = df[col].isnull().sum()
            duplicate_ids = df[col].duplicated().sum()

            id_issues[col] = {
                'null_ids': int(null_ids),
                'duplicate_ids': int(duplicate_ids)
            }

        return {
            'dataset': dataset_name,
            'duplicate_rows': int(duplicate_rows),
            'id_column_issues': id_issues
        }

    def validate_dataset(self, df, dataset_name):
        """Run all validation checks on a dataset"""
        print(f"Validating {dataset_name}...")

        completeness = self.check_completeness(df, dataset_name)
        accuracy = self.check_accuracy(df, dataset_name)
        consistency = self.check_consistency(df, dataset_name)

        # Calculate overall score
        overall_score = (
            completeness['completeness_score'] * 0.4 +
            85 * 0.3 +  # Default accuracy score
            (100 - min(consistency['duplicate_rows'] / len(df) * 100, 100)) * 0.3
        )

        result = {
            'dataset': dataset_name,
            'overall_score': overall_score,
            'completeness': completeness,
            'accuracy': accuracy,
            'consistency': consistency
        }

        self.results[dataset_name] = result

        print(f"  Overall Quality Score: {overall_score:.2f}/100")

        return result

    def generate_report(self):
        """Generate quality validation report"""
        report = "Data Quality Validation Report\\n"
        report += "=" * 50 + "\\n\\n"

        for dataset, result in self.results.items():
            report += f"Dataset: {dataset}\\n"
            report += f"Overall Score: {result['overall_score']:.2f}/100\\n"
            report += "-" * 40 + "\\n"

        return report

# Usage
if __name__ == "__main__":
    validator = DataQualityValidator()
    # validator.validate_dataset(df, "Dataset Name")
    # print(validator.generate_report())
'''

        code_path = os.path.join(self.output_dir, 'quality_checks.py')
        with open(code_path, 'w', encoding='utf-8') as f:
            f.write(code)

        print(f"  ‚úì Saved to: {code_path}")
        return code_path

    def generate_analysis_functions_code(self):
        """Generate core analysis functions"""
        print("\nüìù Generating analysis functions code...")

        code = '''"""
Core Analysis Functions for E-commerce Data
Statistical analysis, pattern discovery, and metrics calculation
"""

import pandas as pd
import numpy as np
from scipy import stats

class EcommerceAnalyzer:
    """Core analysis functions for e-commerce data"""

    def __init__(self, data):
        self.data = data

    def calculate_order_metrics(self, orders_df):
        """Calculate key order metrics"""
        metrics = {}

        if 'order_id' in orders_df.columns:
            metrics['total_orders'] = len(orders_df)

        if 'order_purchase_timestamp' in orders_df.columns:
            orders_df['order_date'] = pd.to_datetime(orders_df['order_purchase_timestamp']).dt.date
            metrics['unique_days'] = orders_df['order_date'].nunique()
            metrics['avg_orders_per_day'] = metrics['total_orders'] / metrics['unique_days']

        if 'customer_id' in orders_df.columns:
            metrics['unique_customers'] = orders_df['customer_id'].nunique()

        return metrics

    def analyze_customer_behavior(self, orders_df, customers_df):
        """Analyze customer behavior patterns"""
        # Merge orders with customers
        merged = pd.merge(orders_df, customers_df, on='customer_id', how='left')

        analysis = {}

        # Orders per customer
        orders_per_customer = merged.groupby('customer_id').size()
        analysis['avg_orders_per_customer'] = orders_per_customer.mean()
        analysis['repeat_customer_rate'] = (orders_per_customer > 1).sum() / len(orders_per_customer)

        # Geographic distribution
        if 'customer_state' in merged.columns:
            analysis['top_states'] = merged['customer_state'].value_counts().head(5).to_dict()

        return analysis

    def analyze_product_performance(self, order_items_df, products_df=None):
        """Analyze product performance metrics"""
        analysis = {}

        if 'product_id' in order_items_df.columns:
            # Sales per product
            product_sales = order_items_df.groupby('product_id').agg({
                'order_id': 'count',
                'price': 'sum'
            }).rename(columns={'order_id': 'order_count', 'price': 'total_revenue'})

            analysis['top_products'] = product_sales.nlargest(10, 'order_count').to_dict()

        if 'price' in order_items_df.columns:
            analysis['avg_product_price'] = order_items_df['price'].mean()
            analysis['median_product_price'] = order_items_df['price'].median()

        return analysis

    def calculate_satisfaction_metrics(self, reviews_df):
        """Calculate customer satisfaction metrics"""
        metrics = {}

        if 'review_score' in reviews_df.columns:
            metrics['avg_review_score'] = reviews_df['review_score'].mean()
            metrics['median_review_score'] = reviews_df['review_score'].median()

            # Score distribution
            score_dist = reviews_df['review_score'].value_counts(normalize=True)
            metrics['positive_review_rate'] = score_dist.get(5, 0) + score_dist.get(4, 0)

        return metrics

    def temporal_analysis(self, orders_df):
        """Analyze temporal patterns in orders"""
        if 'order_purchase_timestamp' not in orders_df.columns:
            return {}

        orders_df['order_purchase_timestamp'] = pd.to_datetime(orders_df['order_purchase_timestamp'])

        analysis = {}

        # By hour
        orders_df['hour'] = orders_df['order_purchase_timestamp'].dt.hour
        analysis['hourly_distribution'] = orders_df.groupby('hour').size().to_dict()

        # By day of week
        orders_df['day_of_week'] = orders_df['order_purchase_timestamp'].dt.dayofweek
        analysis['daily_distribution'] = orders_df.groupby('day_of_week').size().to_dict()

        # By month
        orders_df['month'] = orders_df['order_purchase_timestamp'].dt.to_period('M')
        analysis['monthly_trend'] = orders_df.groupby('month').size().to_dict()

        return analysis

    def payment_analysis(self, payments_df):
        """Analyze payment patterns"""
        analysis = {}

        if 'payment_type' in payments_df.columns:
            analysis['payment_type_distribution'] = payments_df['payment_type'].value_counts(normalize=True).to_dict()

        if 'payment_installments' in payments_df.columns:
            analysis['avg_installments'] = payments_df['payment_installments'].mean()
            analysis['installment_rate'] = (payments_df['payment_installments'] > 1).mean()

        if 'payment_value' in payments_df.columns:
            analysis['avg_payment_value'] = payments_df['payment_value'].mean()
            analysis['total_payment_value'] = payments_df['payment_value'].sum()

        return analysis

    def correlation_analysis(self, df, cols):
        """Perform correlation analysis on specified columns"""
        available_cols = [col for col in cols if col in df.columns]

        if len(available_cols) < 2:
            return {}

        corr_matrix = df[available_cols].corr()

        # Find strong correlations
        strong_corrs = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_val = corr_matrix.iloc[i, j]
                if abs(corr_val) > 0.5:
                    strong_corrs.append({
                        'var1': corr_matrix.columns[i],
                        'var2': corr_matrix.columns[j],
                        'correlation': corr_val
                    })

        return {
            'correlation_matrix': corr_matrix.to_dict(),
            'strong_correlations': strong_corrs
        }

# Usage example
if __name__ == "__main__":
    analyzer = EcommerceAnalyzer(data=None)
    # metrics = analyzer.calculate_order_metrics(orders_df)
    # print(metrics)
'''

        code_path = os.path.join(self.output_dir, 'analysis_functions.py')
        with open(code_path, 'w', encoding='utf-8') as f:
            f.write(code)

        print(f"  ‚úì Saved to: {code_path}")
        return code_path

    def generate_complete_pipeline_code(self):
        """Generate complete analysis pipeline"""
        print("\nüìù Generating complete pipeline code...")

        code = '''"""
Complete E-commerce Analysis Pipeline
End-to-end analysis from raw data to insights
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from data_preprocessing import DataPreprocessor
from quality_checks import DataQualityValidator
from analysis_functions import EcommerceAnalyzer

class CompleteAnalysisPipeline:
    """Complete analysis workflow"""

    def __init__(self, data_dir='data_storage/'):
        self.data_dir = data_dir
        self.preprocessor = DataPreprocessor(data_dir)
        self.validator = DataQualityValidator()
        self.analyzer = None
        self.results = {}

    def run(self):
        """Run complete analysis pipeline"""
        print("="*70)
        print("E-COMMERCE ANALYSIS PIPELINE")
        print("="*70)

        # Stage 1: Load and clean data
        print("\\n[1/5] Loading and cleaning data...")
        self.preprocessor.load_all_data()
        self.preprocessor.clean_orders()
        self.preprocessor.clean_customers()
        self.preprocessor.clean_order_items()
        self.preprocessor.clean_payments()
        self.preprocessor.clean_reviews()
        self.preprocessor.merge_datasets()

        data = self.preprocessor.get_clean_data()

        # Stage 2: Validate data quality
        print("\\n[2/5] Validating data quality...")
        for name, df in data.items():
            if 'Orders' in name or 'Customers' in name:
                self.validator.validate_dataset(df, name)

        # Stage 3: Core analysis
        print("\\n[3/5] Running core analysis...")
        self.analyzer = EcommerceAnalyzer(data)

        if 'Orders_clean' in data:
            self.results['order_metrics'] = self.analyzer.calculate_order_metrics(data['Orders_clean'])
            self.results['temporal_patterns'] = self.analyzer.temporal_analysis(data['Orders_clean'])

        if 'Orders_clean' in data and 'Customers_clean' in data:
            self.results['customer_behavior'] = self.analyzer.analyze_customer_behavior(
                data['Orders_clean'], data['Customers_clean']
            )

        if 'Order_Items_clean' in data:
            self.results['product_performance'] = self.analyzer.analyze_product_performance(data['Order_Items_clean'])

        if 'Reviews_clean' in data:
            self.results['satisfaction'] = self.analyzer.calculate_satisfaction_metrics(data['Reviews_clean'])

        if 'Payments_clean' in data:
            self.results['payment_patterns'] = self.analyzer.payment_analysis(data['Payments_clean'])

        # Stage 4: Generate insights
        print("\\n[4/5] Generating insights...")
        self.generate_insights()

        # Stage 5: Create report
        print("\\n[5/5] Creating report...")
        self.create_summary_report()

        return self.results

    def generate_insights(self):
        """Generate key insights from analysis"""
        self.results['insights'] = []

        # Order trend insight
        if 'order_metrics' in self.results:
            orders = self.results['order_metrics']
            insight = f"Processed {orders.get('total_orders', 0):,} orders from "
            insight += f"{orders.get('unique_customers', 0):,} unique customers"
            self.results['insights'].append(insight)

        # Satisfaction insight
        if 'satisfaction' in self.results:
            sat = self.results['satisfaction']
            avg_score = sat.get('avg_review_score', 0)
            if avg_score > 0:
                insight = f"Average customer satisfaction: {avg_score:.2f}/5.0"
                self.results['insights'].append(insight)

        # Repeat customer insight
        if 'customer_behavior' in self.results:
            repeat_rate = self.results['customer_behavior'].get('repeat_customer_rate', 0)
            insight = f"Repeat customer rate: {repeat_rate*100:.1f}%"
            self.results['insights'].append(insight)

    def create_summary_report(self):
        """Create summary report"""
        report = "\\n" + "="*70
        report += "\\nANALYSIS SUMMARY"
        report += "\\n" + "="*70 + "\\n"

        for key, value in self.results.items():
            if key != 'insights':
                report += f"\\n{key.upper()}:\\n"
                report += str(value) + "\\n"

        report += "\\nKEY INSIGHTS:\\n"
        for i, insight in enumerate(self.results.get('insights', []), 1):
            report += f"{i}. {insight}\\n"

        print(report)

        return report

    def export_results(self, output_dir='complete_analysis/generated_code/'):
        """Export analysis results"""
        import json
        import os

        os.makedirs(output_dir, exist_ok=True)

        # Export results as JSON
        results_path = f"{output_dir}/analysis_results.json"

        # Convert non-serializable objects
        serializable_results = {}
        for key, value in self.results.items():
            if isinstance(value, dict):
                serializable_results[key] = {
                    k: str(v) if not isinstance(v, (int, float, str, list, dict)) else v
                    for k, v in value.items()
                }
            else:
                serializable_results[key] = str(value)

        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)

        print(f"\\n‚úì Results exported to: {results_path}")

# Main execution
if __name__ == "__main__":
    pipeline = CompleteAnalysisPipeline()
    results = pipeline.run()
    pipeline.export_results()

    print("\\n" + "="*70)
    print("‚úÖ ANALYSIS COMPLETE")
    print("="*70)
'''

        code_path = os.path.join(self.output_dir, 'complete_analysis_pipeline.py')
        with open(code_path, 'w', encoding='utf-8') as f:
            f.write(code)

        print(f"  ‚úì Saved to: {code_path}")
        return code_path

    def generate_unit_tests_code(self):
        """Generate unit tests for validation"""
        print("\nüìù Generating unit tests...")

        code = '''"""
Unit Tests for E-commerce Analysis
Test data preprocessing, analysis functions, and quality checks
"""

import unittest
import pandas as pd
import numpy as np
from data_preprocessing import DataPreprocessor
from quality_checks import DataQualityValidator
from analysis_functions import EcommerceAnalyzer

class TestDataPreprocessing(unittest.TestCase):
    """Test data preprocessing functions"""

    def setUp(self):
        """Set up test data"""
        self.preprocessor = DataPreprocessor()

    def test_load_data(self):
        """Test data loading"""
        # This would require actual test data files
        pass

class TestDataQualityValidator(unittest.TestCase):
    """Test data quality validation"""

    def setUp(self):
        """Set up test validator"""
        self.validator = DataQualityValidator()

    def test_completeness_check(self):
        """Test completeness checking"""
        # Create test dataframe with missing values
        test_df = pd.DataFrame({
            'a': [1, 2, np.nan, 4],
            'b': [5, np.nan, 7, 8]
        })

        result = self.validator.check_completeness(test_df, 'test')

        self.assertIn('completeness_score', result)
        self.assertLess(result['completeness_score'], 100)

    def test_consistency_check(self):
        """Test consistency checking"""
        test_df = pd.DataFrame({
            'id': [1, 2, 2, 4],  # Duplicate ID
            'value': [10, 20, 30, 40]
        })

        result = self.validator.check_consistency(test_df, 'test')

        self.assertIn('duplicate_rows', result)
        self.assertEqual(result['duplicate_rows'], 0)  # No full row duplicates

class TestEcommerceAnalyzer(unittest.TestCase):
    """Test core analysis functions"""

    def setUp(self):
        """Set up test data"""
        self.analyzer = EcommerceAnalyzer(data=None)

    def test_order_metrics(self):
        """Test order metrics calculation"""
        test_orders = pd.DataFrame({
            'order_id': ['O1', 'O2', 'O3'],
            'customer_id': ['C1', 'C2', 'C1'],
            'order_purchase_timestamp': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03'])
        })

        metrics = self.analyzer.calculate_order_metrics(test_orders)

        self.assertEqual(metrics['total_orders'], 3)
        self.assertEqual(metrics['unique_customers'], 2)

    def test_temporal_analysis(self):
        """Test temporal analysis"""
        test_orders = pd.DataFrame({
            'order_id': ['O1', 'O2'],
            'order_purchase_timestamp': pd.to_datetime(['2024-01-01 10:00', '2024-01-01 14:00'])
        })

        result = self.analyzer.temporal_analysis(test_orders)

        self.assertIn('hourly_distribution', result)
        self.assertIn('daily_distribution', result)

class TestIntegration(unittest.TestCase):
    """Integration tests"""

    def test_full_pipeline(self):
        """Test complete analysis pipeline"""
        # This would test the entire pipeline
        pass

if __name__ == '__main__':
    unittest.main()
'''

        code_path = os.path.join(self.output_dir, 'test_analysis.py')
        with open(code_path, 'w', encoding='utf-8') as f:
            f.write(code)

        print(f"  ‚úì Saved to: {code_path}")
        return code_path

    def generate_documentation(self):
        """Generate code documentation"""
        print("\nüìù Generating documentation...")

        doc = '''# E-commerce Analysis Code Documentation

## Overview

This directory contains the complete, production-ready code for e-commerce data analysis.

## Files

### 1. data_preprocessing.py
Handles all data loading, cleaning, and preparation tasks.

**Key Classes:**
- `DataPreprocessor`: Main preprocessing class

**Key Methods:**
- `load_all_data()`: Load all CSV datasets
- `clean_orders()`: Clean and preprocess orders data
- `clean_customers()`: Clean customer data
- `clean_order_items()`: Clean order items data
- `clean_payments()`: Clean payment data
- `clean_reviews()`: Clean review data
- `merge_datasets()`: Merge related datasets

**Usage:**
```python
from data_preprocessing import DataPreprocessor

preprocessor = DataPreprocessor('data_storage/')
preprocessor.load_all_data()
preprocessor.clean_orders()
preprocessor.merge_datasets()
data = preprocessor.get_clean_data()
```

### 2. quality_checks.py
Performs comprehensive data quality validation.

**Key Classes:**
- `DataQualityValidator`: Quality validation framework

**Key Methods:**
- `check_completeness()`: Check for missing values
- `check_accuracy()`: Check for outliers and data type issues
- `check_consistency()`: Check for duplicates and ID issues
- `validate_dataset()`: Run all validation checks
- `generate_report()`: Generate quality report

**Usage:**
```python
from quality_checks import DataQualityValidator

validator = DataQualityValidator()
result = validator.validate_dataset(df, "Orders")
print(f"Quality Score: {result['overall_score']:.2f}/100")
```

### 3. analysis_functions.py
Core analysis functions for e-commerce metrics.

**Key Classes:**
- `EcommerceAnalyzer`: Statistical analysis and metrics

**Key Methods:**
- `calculate_order_metrics()`: Order volume and frequency metrics
- `analyze_customer_behavior()`: Customer patterns and segmentation
- `analyze_product_performance()`: Product sales and revenue
- `calculate_satisfaction_metrics()`: Review scores and satisfaction
- `temporal_analysis()`: Time-based patterns
- `payment_analysis()`: Payment method patterns
- `correlation_analysis()`: Variable correlations

**Usage:**
```python
from analysis_functions import EcommerceAnalyzer

analyzer = EcommerceAnalyzer(data)
metrics = analyzer.calculate_order_metrics(orders_df)
behavior = analyzer.analyze_customer_behavior(orders_df, customers_df)
```

### 4. complete_analysis_pipeline.py
End-to-end analysis workflow.

**Key Classes:**
- `CompleteAnalysisPipeline`: Full analysis automation

**Key Methods:**
- `run()`: Execute complete pipeline
- `generate_insights()`: Extract key insights
- `create_summary_report()`: Generate summary
- `export_results()`: Export results as JSON

**Usage:**
```python
from complete_analysis_pipeline import CompleteAnalysisPipeline

pipeline = CompleteAnalysisPipeline('data_storage/')
results = pipeline.run()
pipeline.export_results()
```

### 5. test_analysis.py
Unit tests for validation.

**Run Tests:**
```bash
python -m unittest test_analysis.py
```

## Requirements

```
pandas>=1.3.0
numpy>=1.21.0
scipy>=1.7.0
```

## Installation

```bash
pip install -r requirements.txt
```

## Running the Analysis

### Quick Start
```bash
python complete_analysis_pipeline.py
```

### Step-by-Step
```python
# 1. Preprocess data
python data_preprocessing.py

# 2. Validate quality
python quality_checks.py

# 3. Run analysis
python analysis_functions.py

# 4. Full pipeline
python complete_analysis_pipeline.py
```

## Output Structure

```
complete_analysis/generated_code/
‚îú‚îÄ‚îÄ data_preprocessing.py
‚îú‚îÄ‚îÄ quality_checks.py
‚îú‚îÄ‚îÄ analysis_functions.py
‚îú‚îÄ‚îÄ complete_analysis_pipeline.py
‚îú‚îÄ‚îÄ test_analysis.py
‚îú‚îÄ‚îÄ README.md (this file)
‚îî‚îÄ‚îÄ analysis_results.json (generated after running)
```

## Best Practices

1. **Always validate data quality** before analysis
2. **Use cleaned data** from preprocessing module
3. **Run tests** after any code modifications
4. **Check results** after each pipeline stage

## Troubleshooting

**Issue: Missing datasets**
- Ensure all CSV files are in `data_storage/` directory

**Issue: Low quality scores**
- Check data quality report for specific issues
- Run preprocessing to clean data

**Issue: Import errors**
- Install required dependencies: `pip install -r requirements.txt`

## Support

For issues or questions, refer to the main analysis report or documentation.
'''

        doc_path = os.path.join(self.output_dir, 'README.md')
        with open(doc_path, 'w', encoding='utf-8') as f:
            f.write(doc)

        print(f"  ‚úì Saved to: {doc_path}")
        return doc_path

    def run_code_generation(self):
        """Run complete code generation workflow"""
        print("\n" + "="*70)
        print("üíª STAGE 5: CODE GENERATION")
        print("="*70)

        # Generate all code modules
        preprocessing = self.generate_data_preprocessing_code()
        quality_checks = self.generate_quality_checks_code()
        analysis_functions = self.generate_analysis_functions_code()
        pipeline = self.generate_complete_pipeline_code()
        tests = self.generate_unit_tests_code()
        documentation = self.generate_documentation()

        # Generate requirements.txt
        print("\nüìù Generating requirements.txt...")
        requirements = '''pandas>=1.3.0
numpy>=1.21.0
scipy>=1.7.0
matplotlib>=3.5.0
seaborn>=0.11.0
'''

        req_path = os.path.join(self.output_dir, 'requirements.txt')
        with open(req_path, 'w', encoding='utf-8') as f:
            f.write(requirements)
        print(f"  ‚úì Saved to: {req_path}")

        return {
            'preprocessing': preprocessing,
            'quality_checks': quality_checks,
            'analysis_functions': analysis_functions,
            'pipeline': pipeline,
            'tests': tests,
            'documentation': documentation,
            'requirements': req_path
        }

    def print_summary(self):
        """Print code generation summary"""
        print("\n" + "="*70)
        print("üìã CODE GENERATION SUMMARY")
        print("="*70)

        print("\nüìÅ Generated Files:")
        print(f"  ‚Ä¢ data_preprocessing.py - Data loading and cleaning")
        print(f"  ‚Ä¢ quality_checks.py - Data quality validation")
        print(f"  ‚Ä¢ analysis_functions.py - Core analysis functions")
        print(f"  ‚Ä¢ complete_analysis_pipeline.py - End-to-end pipeline")
        print(f"  ‚Ä¢ test_analysis.py - Unit tests")
        print(f"  ‚Ä¢ requirements.txt - Dependencies")
        print(f"  ‚Ä¢ README.md - Documentation")

        print(f"\nüìÇ All code saved to: {self.output_dir}")

        print("\n" + "="*70)

if __name__ == "__main__":
    # Run code generation
    generator = AnalysisCodeGenerator()
    code_files = generator.run_code_generation()
    generator.print_summary()

    print("\n‚úÖ STAGE 5 COMPLETE: Code Generation")
    print("  All analysis code generated successfully")
    print("  Proceeding to Stage 6: Report Generation...")
